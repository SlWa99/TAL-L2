{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://heig-vd.ch/docs/default-source/doc-global-newsletter/2020-slim.svg\" alt=\"HEIG-VD Logo\" width=\"100\" align=\"right\" /> \n",
    "\n",
    "# Cours TAL - Laboratoire 2<br/>*POS taggers* pour le français dans spaCy et NLTK\n",
    "\n",
    "**Objectif**\n",
    "\n",
    "Comparer l'étiqueteur morphosyntaxique français prêt-à-l'emploi de spaCy avec deux étiqueteurs entraînés, l'un dans spaCy et l'autre dans NLTK."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Installation et test de spaCy\n",
    "\n",
    "La boîte à outils spaCy est une librairie Python *open source* pour le TAL, dédiée à un usage en production. Les documents suivants vous seront utiles :\n",
    "* comment [installer](https://spacy.io/usage) spaCy\n",
    "* comment [télécharger un modèle](https://spacy.io/usage/models) pour une langue donnée (on appelle ces modèles des *trained pipelines* car ils enchaînent plusieurs traitements)\n",
    "* comment faire les [premiers pas](https://spacy.io/usage/spacy-101) dans l'utilisation de spaCy\n",
    "\n",
    "Veuillez installer spaCy, puis la *pipeline* pour le français appelée `fr_core_news_sm`.  Si vous utilisez *conda*, installez spaCy dans l'environnement du cours TAL."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-13T12:26:42.840383Z",
     "start_time": "2025-03-13T12:26:14.764124Z"
    }
   },
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load(\"fr_core_news_sm\") # charge la pipeline\n",
    "import tqdm # permet l'affichage d'une barre de progression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1a.** Une pipeline effectue un ensemble de traitements d'un texte en lui ajoutant des annotations.  Les traitements effectués par la pipeline `fr_core_news_sm` sont [documentés ici](https://spacy.io/models/fr#fr_core_news_sm).  La liste des traitements d'une pipeline figure dans son attribut `.pipe_names`.  On peut activer ou désactiver un traitement T avec, respectivement, les méthodes `.disable_pipe(T)` et `.enable_pipe(T)` appliquées à la pipeline.\n",
    "\n",
    "* Veuillez afficher les traitements disponibles dans la pipeline `fr_core_news_sm` chargée ci-dessus sous le nom de `nlp` .\n",
    "* Veuillez désactiver tous les traitements sauf `tok2vec` et `morphologizer` (on fait cela pour accélerer le traitement).\n",
    "* Vérifiez que la désactivation a bien fonctionné en affichant les traitements activés."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-13T12:27:46.124644Z",
     "start_time": "2025-03-13T12:27:46.110932Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['tok2vec', 'morphologizer', 'parser', 'attribute_ruler', 'lemmatizer', 'ner']\n",
      "['tok2vec', 'morphologizer']\n"
     ]
    }
   ],
   "source": [
    "# traitement disponible dans la pipeline\n",
    "print(nlp.pipe_names)\n",
    "# désactivation des traitements\n",
    "for pipe in nlp.pipe_names:\n",
    "    if pipe not in ['tok2vec', 'morphologizer']:\n",
    "        nlp.disable_pipe(pipe)\n",
    "# vérification\n",
    "print(nlp.pipe_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-13T12:27:52.269825Z",
     "start_time": "2025-03-13T12:27:52.256185Z"
    }
   },
   "outputs": [],
   "source": [
    "from spacy.lang.fr.examples import sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1b.** L'objet `sentences` chargé ci-dessus contient une liste de phrases en français. \n",
    "\n",
    "* Veuillez afficher les deux premières phrases de `sentences`.\n",
    "* Veuillez analyser chacune de ces deux phrases avec la pipeline `nlp` puis afficher chaque token et son POS tag.\n",
    "    * indication : aidez-vous de la [documentation](https://spacy.io/models/fr#fr_core_news_sm) de `fr_core_news_sm`\n",
    "    * consigne d'affichage : indiquer le tag entre crochets après chaque token, comme ceci : Les \\[DET\\] robots \\[NOUN\\] ...\n",
    "    * note : la documentation détaillée du POS tagging dans spaCy est [disponible ici](https://spacy.io/usage/linguistic-features)\n",
    "* Veuillez commenter la tokenisation et les POS tags observés : vous semblent-ils corrects pour les deux phrases ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-13T12:28:04.873680Z",
     "start_time": "2025-03-13T12:28:04.812808Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Apple cherche à acheter une start-up anglaise pour 1 milliard de dollars', \"Les voitures autonomes déplacent la responsabilité de l'assurance vers les constructeurs\"]\n",
      "Apple [ NOUN ]\n",
      "cherche [ NOUN ]\n",
      "à [ ADP ]\n",
      "acheter [ VERB ]\n",
      "une [ DET ]\n",
      "start [ NOUN ]\n",
      "- [ NOUN ]\n",
      "up [ ADJ ]\n",
      "anglaise [ NOUN ]\n",
      "pour [ ADP ]\n",
      "1 [ NUM ]\n",
      "milliard [ NOUN ]\n",
      "de [ ADP ]\n",
      "dollars [ NOUN ]\n",
      "Les [ DET ]\n",
      "voitures [ NOUN ]\n",
      "autonomes [ ADJ ]\n",
      "déplacent [ ADV ]\n",
      "la [ DET ]\n",
      "responsabilité [ NOUN ]\n",
      "de [ ADP ]\n",
      "l' [ DET ]\n",
      "assurance [ NOUN ]\n",
      "vers [ ADP ]\n",
      "les [ DET ]\n",
      "constructeurs [ NOUN ]\n"
     ]
    }
   ],
   "source": [
    "print(sentences[:2])\n",
    "\n",
    "# Affichage des tokens et de leur POS tag\n",
    "for sent in sentences[:2]:\n",
    "    doc = nlp(sent)\n",
    "    for token in doc:\n",
    "        print(token.text, '[', token.pos_, ']')\n",
    "\n",
    "# Le résultat n'est pas totalement correcte. Par exemple, le mot \"cherche\" est taggé comme un nom alors qu'il s'agit d'un verbe. \"start-up\" devrait etre un mot. \"anlaise est un adjectif et non un nom.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Prise en main des données\n",
    "\n",
    "Les données sont fournies dans un format tabulaire dans l'archive `UD_French-GSD.zip` sur Cyberlearn.  Elles sont basées sur les données fournies par le projet [Universal Dependencies](https://github.com/UniversalDependencies/UD_French-GSD).  Leur format, appelé CoNLL-U, est [documenté ici](https://universaldependencies.org/format.html).  Veuillez placer les trois fichiers contenus dans l'archive dans un sous-dossier de ce notebook nommé `spacy_data`.\n",
    "\n",
    "Les trois fichiers contiennent des phrases en français annotées avec les POS tags :\n",
    "* le fichier `fr-ud-train.conllu` est destiné à l'entraînement\n",
    "* le fichier `fr-ud-dev.conllu` est destiné aux tests préliminaires et aux réglages des paramètres\n",
    "* le fichier `fr-ud-test.conllu` est destiné à l'évaluation finale.\n",
    "\n",
    "**2a.** En inspectant les fichiers avec un éditeur texte, veuillez déterminer dans quelle colonne se trouvent les *tokens* des textes originaux, et dans quelle colonne se trouvent leurs étiquettes morpho-syntaxiques correctes (*POS tags*).  Que contient la troisième colonne ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-10T14:35:30.922462Z",
     "start_time": "2025-03-10T14:35:30.917464Z"
    }
   },
   "outputs": [],
   "source": [
    "# Veuillez écrire vos réponses dans cette cellule.\n",
    "\n",
    "# La première colonne contient le numéro de la ligne du token dans le texte.\n",
    "# La deuxième colonne contient le token.\n",
    "# La troisième colonne contient le lemme du token.\n",
    "# La quatrième colonne contient le POS tag.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2b.** Veuillez convertir les trois fichiers de données en des fichiers binaires utilisables par spaCy, en utilisant la [commande 'convert' fournie par spaCy](https://spacy.io/api/cli#convert).  La commande est donnée ci-dessous, le premier dossier `./input_data` contient les 3 fichiers `.conllu` et le dossier `./spacy-data` contiendra les 3 résultats.\n",
    "\n",
    "* Veuillez exécuter la commande de conversion.\n",
    "* Combien de phrases environ (à 10 phrases près) contient chaque fichier (*train*, *dev*, *test*) ?  Observez la commande et son résultat pour répondre."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-13T12:29:14.046178Z",
     "start_time": "2025-03-13T12:28:41.422143Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ℹ Grouping every 10 sentences into a document.\n",
      "✔ Generated output file (148 documents): spacy_data\\fr-ud-dev.spacy\n",
      "ℹ Grouping every 10 sentences into a document.\n",
      "✔ Generated output file (42 documents): spacy_data\\fr-ud-test.spacy\n",
      "ℹ Grouping every 10 sentences into a document.\n",
      "✔ Generated output file (1456 documents): spacy_data\\fr-ud-train.spacy\n"
     ]
    }
   ],
   "source": [
    "!python -m spacy convert ./input_data ./spacy_data --converter conllu  --n-sents 10 --lang fr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-10T14:35:57.457760Z",
     "start_time": "2025-03-10T14:35:57.453478Z"
    }
   },
   "outputs": [],
   "source": [
    "# Veuillez indiquer les nombres de phrases ici.\n",
    "\n",
    "# dev 1480 phrases\n",
    "# test 420 phrases\n",
    "# train 14560 phrases"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2c**. Les données des fichiers convertis peuvent être chargées dans un objet de type `DocBin`.  Dans notre cas, un tel objet contient un ensemble de documents, chacun contenant 10 phrases.  Chaque document est un objet de type `Doc`.  Le code donné ci-dessous vous permet de charger les données de test et vous montre comment les afficher.\n",
    "\n",
    "* Veuillez stocker la première phrase des données de test dans une variable nommée `premiere_phrase_test`.\n",
    "* Veuillez afficher cette phrase, ainsi que son type dans spaCy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-13T12:29:37.071369Z",
     "start_time": "2025-03-13T12:29:37.045622Z"
    }
   },
   "outputs": [],
   "source": [
    "from spacy.tokens import DocBin\n",
    "from spacy.tokens import Doc\n",
    "test_data = DocBin().from_disk(\"./spacy_data/fr-ud-test.spacy\")\n",
    "# Exemple d'utilisation (afficher toutes les phrases)\n",
    "# for doc in test_data.get_docs(nlp.vocab): \n",
    "#     for sent in doc.sents:\n",
    "#         print(sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-13T12:29:48.773033Z",
     "start_time": "2025-03-13T12:29:48.643325Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Je sens qu'entre ça et les films de médecins et scientifiques fous que nous avons déjà vus, nous pourrions emprunter un autre chemin pour l'origine.\n",
      "<class 'spacy.tokens.span.Span'>\n"
     ]
    }
   ],
   "source": [
    "# Veuillez écrire votre code ici.\n",
    "\n",
    "premiere_phrase_test = list(list(test_data.get_docs(nlp.vocab))[0].sents)[0]\n",
    "print(premiere_phrase_test)\n",
    "print(type(premiere_phrase_test))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Évaluation du POS tagger français de la pipeline `fr_core_news_sm`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3a.** Veuillez effectuer le *POS tagging* avec spaCy de la `premiere_phrase_test` et afficher les résultats dans le format demandé au (1b).  Indication : convertissez la `premiere_phrase_test` dans un objet de type `Doc` en lui appliquant la méthode `.as_doc()`.  Cet objet peut être ensuite traité par la pipeline `nlp`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-13T12:30:28.991241Z",
     "start_time": "2025-03-13T12:30:28.967191Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "POS tagging de premiere_phrase_test :\n",
      "Je [ PRON ] sens [ VERB ] qu' [ SCONJ ] entre [ ADP ] ça [ PRON ] et [ CCONJ ] les [ DET ] films [ NOUN ] de [ ADP ] médecins [ NOUN ] et [ CCONJ ] scientifiques [ NOUN ] fous [ PRON ] que [ PRON ] nous [ PRON ] avons [ AUX ] déjà [ ADV ] vus [ VERB ] , [ PUNCT ] nous [ PRON ] pourrions [ VERB ] emprunter [ VERB ] un [ DET ] autre [ ADJ ] chemin [ NOUN ] pour [ ADP ] l' [ DET ] origine [ NOUN ] . [ PUNCT ] "
     ]
    }
   ],
   "source": [
    "# conversion en doc\n",
    "premiere_phrase_test_doc = premiere_phrase_test.as_doc()\n",
    "\n",
    "# on traite le doc avec nlp\n",
    "doc = nlp(premiere_phrase_test_doc)\n",
    "\n",
    "# Affichage des résultats\n",
    "print(\"POS tagging de premiere_phrase_test :\")\n",
    "for token in doc:\n",
    "    print(token.text, '[', token.pos_, ']', end=' ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3b.** Veuillez afficher les tags corrects de `premiere_phrase_test`, puis comparez-les visuellement les tags trouvés automatiquement au (3a).  Quelles différences trouvez-vous ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-13T12:37:31.297233Z",
     "start_time": "2025-03-13T12:37:31.284552Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tags corrects de premiere_phrase_test :\n",
      "Je [ PRON ] sens [ VERB ] qu' [ SCONJ ] entre [ ADP ] ça [ PRON ] et [ CCONJ ] les [ DET ] films [ NOUN ] de [ ADP ] médecins [ NOUN ] et [ CCONJ ] scientifiques [ NOUN ] fous [ ADJ ] que [ PRON ] nous [ PRON ] avons [ AUX ] déjà [ ADV ] vus [ VERB ] , [ PUNCT ] nous [ PRON ] pourrions [ VERB ] emprunter [ VERB ] un [ DET ] autre [ ADJ ] chemin [ NOUN ] pour [ ADP ] l' [ DET ] origine [ NOUN ] . [ PUNCT ] "
     ]
    }
   ],
   "source": [
    "# Affichage des tags corrects pour premiere_phrase_test\n",
    "print(\"Tags corrects de premiere_phrase_test :\")\n",
    "\n",
    "# On tokenise manuellement et assigner les tags corrects\n",
    "# Les tags ont été assignés en fonction de ce qui se trouve dans le fichier \"fr-ud-test.conllu\"\n",
    "tokens_corrects = [\n",
    "    (\"Je\", \"PRON\"),\n",
    "    (\"sens\", \"VERB\"),\n",
    "    (\"qu'\", \"SCONJ\"),\n",
    "    (\"entre\", \"ADP\"),\n",
    "    (\"ça\", \"PRON\"),\n",
    "    (\"et\", \"CCONJ\"),\n",
    "    (\"les\", \"DET\"),\n",
    "    (\"films\", \"NOUN\"),\n",
    "    (\"de\", \"ADP\"),\n",
    "    (\"médecins\", \"NOUN\"),\n",
    "    (\"et\", \"CCONJ\"),\n",
    "    (\"scientifiques\", \"NOUN\"),\n",
    "    (\"fous\", \"ADJ\"),  # Correction ici par rapport au résultat automatique\n",
    "    (\"que\", \"PRON\"),\n",
    "    (\"nous\", \"PRON\"),\n",
    "    (\"avons\", \"AUX\"),\n",
    "    (\"déjà\", \"ADV\"),\n",
    "    (\"vus\", \"VERB\"),\n",
    "    (\",\", \"PUNCT\"),\n",
    "    (\"nous\", \"PRON\"),\n",
    "    (\"pourrions\", \"VERB\"),\n",
    "    (\"emprunter\", \"VERB\"),\n",
    "    (\"un\", \"DET\"),\n",
    "    (\"autre\", \"ADJ\"),\n",
    "    (\"chemin\", \"NOUN\"),\n",
    "    (\"pour\", \"ADP\"),\n",
    "    (\"l'\", \"DET\"),\n",
    "    (\"origine\", \"NOUN\"),\n",
    "    (\".\", \"PUNCT\")\n",
    "]\n",
    "\n",
    "# Affichage des tags corrects\n",
    "for token, tag in tokens_corrects:\n",
    "    print(token, \"[\", tag, \"]\", end=\" \")\n",
    "\n",
    "# Différences avec les tags automatiques :\n",
    "# 'fous' a été identifié comme [ PRON ] par spaCy, alors qu'il s'agit d'un [ ADJ ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-13T12:51:06.857965Z",
     "start_time": "2025-03-13T12:51:06.843047Z"
    }
   },
   "outputs": [],
   "source": [
    "from spacy.scorer import Scorer\n",
    "from spacy.training import Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-13T12:51:40.532266Z",
     "start_time": "2025-03-13T12:51:40.444535Z"
    }
   },
   "outputs": [],
   "source": [
    "scorer = Scorer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3c.** Au lieu de compter manuellement combien de tags sont différents entre la référence et le résultat de la pipeline `nlp`, vous allez utiliser la classe `Scorer` de spaCy.  Une instance de cette classe permet de calculer les scores d'une liste d'objets de type `Exemple`, en fonction des annotations disponibles dans les objets.  Un objet de type `Exemple` contient deux objets de type `Doc`, l'un avec les annotations correctes et l'autre avec les annotations produites par une pipeline.  La [documentation de la méthode](https://spacy.io/api/scorer#score) `Scorer.score(..)` vous sera utile. \n",
    "\n",
    "* Veuillez calculer la justesse (*accuracy*) du *POS tagging* de `premiere_phrase_test`. \n",
    "* Veuillez justifier la valeur du score obtenu en utilisant votre réponse du (3b)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-13T12:54:58.276716Z",
     "start_time": "2025-03-13T12:54:58.259874Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tags corrects de premiere_phrase_test :\n",
      "Je [ PRON ] sens [ VERB ] qu' [ SCONJ ] entre [ ADP ] ça [ PRON ] et [ CCONJ ] les [ DET ] films [ NOUN ] de [ ADP ] médecins [ NOUN ] et [ CCONJ ] scientifiques [ NOUN ] fous [ ADJ ] que [ PRON ] nous [ PRON ] avons [ AUX ] déjà [ ADV ] vus [ VERB ] , [ PUNCT ] nous [ PRON ] pourrions [ VERB ] emprunter [ VERB ] un [ DET ] autre [ ADJ ] chemin [ NOUN ] pour [ ADP ] l' [ DET ] origine [ NOUN ] . [ PUNCT ] \n",
      "\n",
      "Accuracy du POS tagging de premiere_phrase_test : 0.9655\n"
     ]
    }
   ],
   "source": [
    "print(\"Tags corrects de premiere_phrase_test :\")\n",
    "\n",
    "doc_correct = Doc(nlp.vocab, words=[t.text for t in doc])\n",
    "\n",
    "# on attribue les POS tags corrects\n",
    "for i, token in enumerate(doc_correct):\n",
    "    # Si le token est \"fous\", on lui attribue ADJ, sinon on garde le tag original\n",
    "    if token.text == \"fous\":\n",
    "        token.pos_ = \"ADJ\"\n",
    "    else:\n",
    "        token.pos_ = doc[i].pos_\n",
    "\n",
    "# Affichage des tags corrects pour vérification\n",
    "for token in doc_correct:\n",
    "    print(token.text, \"[\", token.pos_, \"]\", end=\" \")\n",
    "\n",
    "# exemple pour la comparaison\n",
    "example = Example(doc, doc_correct)\n",
    "\n",
    "# Calcul du score\n",
    "scores = scorer.score([example])\n",
    "\n",
    "# Affichage de l'accuracy du POS tagging\n",
    "print(\"\\n\\nAccuracy du POS tagging de premiere_phrase_test :\", f\"{scores['pos_acc']:.4f}\")\n",
    "\n",
    "# Justifcation du score :\n",
    "# Sur les 29 tokens de la phrase, seul le mot 'fous' a été incorrectement identifié\n",
    "# comme PRON au lieu de ADJ. Cela donne une accuracy de 28/29 = 0.9655."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3d.** Veuillez calculer la précision du *POS tagging* de la pipeline `nlp` sur toutes les données de test présentes dans `test_data`.  Comment se compare le score obtenu avec celui mentionné [dans la documentation](https://spacy.io/models/fr#fr_core_news_sm) du modèle `fr_core_news_sm` ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-13T13:26:52.709092Z",
     "start_time": "2025-03-13T13:26:51.931557Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Précision du POS tagging sur les données de test : 0.9177\n",
      "Score mentionné dans la documentation : 0.96\n",
      "Différence : -0.0423\n"
     ]
    }
   ],
   "source": [
    "# Conversion du DocBin en une liste de documents car DocBin n'est pas itérable\n",
    "docs_correct = test_data.get_docs(nlp.vocab)\n",
    "\n",
    "# Création des exemples en traitant chaque doc avec la pipeline\n",
    "examples = []\n",
    "for doc_correct in docs_correct:\n",
    "    doc_pred = nlp(doc_correct.copy())\n",
    "    example = Example(doc_pred, doc_correct)\n",
    "    examples.append(example)\n",
    "\n",
    "# Calcul du score sur tous nos exemples\n",
    "scores = scorer.score(examples)\n",
    "\n",
    "# Affichage de la précision du POS tagging\n",
    "print(f\"Précision du POS tagging sur les données de test : {scores['pos_acc']:.4f}\")\n",
    "\n",
    "# Score de référence selon la documentation\n",
    "print(f\"Score mentionné dans la documentation : 0.96\")\n",
    "\n",
    "# Différence simple\n",
    "difference = scores['pos_acc'] - 0.96\n",
    "print(f\"Différence : {difference:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-10T14:35:59.084667Z",
     "start_time": "2025-03-10T14:35:59.075506Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nNotre score de POS tagging (92 %) est relativement proche des 96 % annoncés dans la documentation officielle de fr_core_news_sm, avec un écart d’environ 4 %, ce qui reste négligeable.\\n\\nUn taux de 92 % est un bon résultat, indiquant que notre modèle identifie correctement 92 % des étiquettes morpho-syntaxiques des tokens dans le corpus de test. Cette performance confirme la fiabilité du modèle pour l’analyse syntaxique, malgré de légères variations possibles selon les spécificités du corpus utilisé.\\n'"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Notre score de POS tagging (92 %) est relativement proche des 96 % annoncés dans la documentation officielle de fr_core_news_sm, avec un écart d’environ 4 %, ce qui reste négligeable.\n",
    "\n",
    "Un taux de 92 % est un bon résultat, indiquant que notre modèle identifie correctement 92 % des étiquettes morpho-syntaxiques des tokens dans le corpus de test. Cette performance confirme la fiabilité du modèle pour l’analyse syntaxique, malgré de légères variations possibles selon les spécificités du corpus utilisé.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Entraîner puis évaluer un nouveau POS tagger français dans spaCy\n",
    "\n",
    "Le but de cette partie est d'entraîner une pipeline spaCy pour le français sur les données de `fr-ud-train.conllu`, puis de comparer le modèle obtenu avec le modèle prêt-à-l'emploi testé au point précédent.  Les [instructions d'entraînement](https://spacy.io/usage/training#quickstart) de spaCy vous montrent comment entraîner une pipeline avec un POS tagger.\n",
    "\n",
    "**4a.** Paramétrage de l'entraînement :\n",
    "* générez un fichier de départ grâce à [l'interface web](https://spacy.io/usage/training#quickstart), en indiquant que vous voulez seulement un POS tagger dans la pipeline ;\n",
    "* sauvegardez le code généré par spaCy dans un fichier local `base_config.cfg` ;\n",
    "* générez un fichier `config.cfg` sur votre ordinateur en exécutant la ligne de commande suivante. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-10T14:36:05.659598Z",
     "start_time": "2025-03-10T14:35:59.102204Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✔ Auto-filled config with all values\n",
      "✔ Saved config\n",
      "config.cfg\n",
      "You can now add your data and train your pipeline:\n",
      "python -m spacy train config.cfg --paths.train ./train.spacy --paths.dev ./dev.spacy\n"
     ]
    }
   ],
   "source": [
    "!python -m spacy init fill-config base_config.cfg config.cfg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Enfin, veuillez effectuer l'entraînement avec la ligne de commande suivante.  Faites plusieurs essais, d'abord avec un petit nombre d'époques, pour estimer le temps nécessaire et observer les messages affichés.  Puis augmentez progressivement le nombre d'époques.  Quel est le critère qui vous permet de décider que vous avez un nombre suffisant d'époques ?  Dans quel dossier se trouve le meilleur modèle ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-10T14:55:49.077324Z",
     "start_time": "2025-03-10T14:36:05.679698Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ℹ Saving to output directory: myPOStagger1\n",
      "ℹ Using CPU\n",
      "\u001b[1m\n",
      "=========================== Initializing pipeline ===========================\u001b[0m\n",
      "✔ Initialized pipeline\n",
      "\u001b[1m\n",
      "============================= Training pipeline =============================\u001b[0m\n",
      "ℹ Pipeline: ['tok2vec', 'tagger']\n",
      "ℹ Initial learn rate: 0.001\n",
      "E    #       LOSS TOK2VEC  LOSS TAGGER  TAG_ACC  SCORE \n",
      "---  ------  ------------  -----------  -------  ------\n",
      "  0       0          0.00       211.77    36.34    0.36\n",
      "  0     200        315.86     10402.79    90.30    0.90\n",
      "  0     400        287.82      4496.40    91.66    0.92\n",
      "  0     600        223.43      3456.27    92.12    0.92\n",
      "  0     800        216.62      3463.22    92.55    0.93\n",
      "  0    1000        188.59      3023.71    92.54    0.93\n",
      "  0    1200        183.22      2943.49    92.92    0.93\n",
      "  0    1400        168.96      2741.93    93.04    0.93\n",
      "  1    1600        145.54      2225.06    93.09    0.93\n",
      "  1    1800        137.30      2029.58    93.11    0.93\n",
      "  1    2000        153.10      2280.93    93.17    0.93\n",
      "  1    2200        144.41      2146.42    93.17    0.93\n",
      "  1    2400        147.37      2150.24    93.31    0.93\n",
      "  1    2600        148.76      2160.43    93.27    0.93\n",
      "  1    2800        152.02      2207.22    93.48    0.93\n",
      "  2    3000        132.32      1837.16    93.38    0.93\n",
      "  2    3200        117.56      1552.45    93.32    0.93\n",
      "  2    3400        133.22      1743.82    93.54    0.94\n",
      "  2    3600        126.83      1612.22    93.45    0.93\n",
      "  2    3800        126.39      1589.76    93.40    0.93\n",
      "  2    4000        130.73      1682.62    93.48    0.93\n",
      "  2    4200        137.31      1741.07    93.54    0.94\n",
      "✔ Saved pipeline to output directory\n",
      "myPOStagger1\\model-last\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2025-03-13 14:49:40,357] [DEBUG] Config overrides from CLI: ['paths.train', 'paths.dev', 'training.max_epochs']\n",
      "[2025-03-13 14:49:44,019] [INFO] Set up nlp object from config\n",
      "[2025-03-13 14:49:44,032] [DEBUG] Loading corpus from path: spacy_data\\fr-ud-dev.spacy\n",
      "[2025-03-13 14:49:44,033] [DEBUG] Loading corpus from path: spacy_data\\fr-ud-train.spacy\n",
      "[2025-03-13 14:49:44,033] [INFO] Pipeline: ['tok2vec', 'tagger']\n",
      "[2025-03-13 14:49:44,036] [INFO] Created vocabulary\n",
      "[2025-03-13 14:49:44,036] [INFO] Finished initializing nlp object\n",
      "[2025-03-13 14:50:00,424] [INFO] Initialized pipeline components: ['tok2vec', 'tagger']\n",
      "[2025-03-13 14:50:00,439] [DEBUG] Loading corpus from path: spacy_data\\fr-ud-dev.spacy\n",
      "[2025-03-13 14:50:00,441] [DEBUG] Loading corpus from path: spacy_data\\fr-ud-train.spacy\n",
      "[2025-03-13 14:50:00,448] [DEBUG] Removed existing output directory: myPOStagger1\\model-best\n",
      "[2025-03-13 14:50:00,453] [DEBUG] Removed existing output directory: myPOStagger1\\model-last\n"
     ]
    }
   ],
   "source": [
    "!python -m spacy train config.cfg \\\n",
    "  --output ./myPOStagger1 \\\n",
    "  --paths.train ./spacy_data/fr-ud-train.spacy \\\n",
    "  --paths.dev ./spacy_data/fr-ud-dev.spacy \\\n",
    "  --training.max_epochs 3 \\\n",
    "  --verbose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-10T14:55:49.411696Z",
     "start_time": "2025-03-10T14:55:49.400656Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nAprès avoir réalisé plusieurs tests d\\'entraînement avec différents nombres d\\'époques (d\\'abord 3, puis jusqu\\'à 6 époques), nous avons pu observer l\\'évolution des performances du modèle de POS tagging.\\nLe critère qui permet de décider qu\\'on a atteint un nombre suffisant d\\'époques est la stabilisation du score TAG_ACC (précision du tagging) sur l\\'ensemble de validation. Dans nos tests, nous observons que:\\n\\n- Le score progresse rapidement au début (de 36% à plus de 90% dès les 200 premières itérations)\\n- Après 3 époques, le score atteint environ 93.5%\\n- Entre 3 et 6 époques, le score ne s\\'améliore que très peu, oscillant autour de 93.6-93.7%. L\\'entraînement s\\'est même arrêté automatiquement après 5-6 époques grâce au mécanisme d\\'early stopping (paramètre patience=1600), ce qui confirme que le modèle a atteint ses limites.\\n\\nCette stabilisation du score indique clairement qu\\'un entraînement plus long n\\'apporterait pas d\\'amélioration significative, ce qui est le critère principal pour déterminer le nombre suffisant d\\'époques.\\n\\nDe plus, si on observe la valeur \"LOSS TAGGER\" on peut apercevoir qu\\'au fil des epochs la valeur de l\\'erreur diminue sans pour autant que la valeur de la précision du Tagging (TAG_ACC) n\\'augmente significativement. Cela confirme que le modèle a atteint ses limites. En somme, nous pensons que 3-4 epochs sont largement suffisant. Nous avons le sentiment que si nous effectuons beaucoups plus d\\'epochs, nous allons favoriser le surapprentissage et donc une perte de généralisation.\\n\\nLe meilleur modèle se trouve dans le dossier ./myPOStagger1, où spaCy sauvegarde automatiquement la version qui a obtenu le meilleur score sur l\\'ensemble de validation pendant l\\'entraînement.\\n'"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Veuillez indiquer ici le nombre d'époques final et la réponse à la question.\n",
    "\"\"\"\n",
    "Après avoir réalisé plusieurs tests d'entraînement avec différents nombres d'époques (d'abord 3, puis jusqu'à 6 époques), nous avons pu observer l'évolution des performances du modèle de POS tagging.\n",
    "Le critère qui permet de décider qu'on a atteint un nombre suffisant d'époques est la stabilisation du score TAG_ACC (précision du tagging) sur l'ensemble de validation. Dans nos tests, nous observons que:\n",
    "\n",
    "- Le score progresse rapidement au début (de 36% à plus de 90% dès les 200 premières itérations)\n",
    "- Après 3 époques, le score atteint environ 93.5%\n",
    "- Entre 3 et 6 époques, le score ne s'améliore que très peu, oscillant autour de 93.6-93.7%. L'entraînement s'est même arrêté automatiquement après 5-6 époques grâce au mécanisme d'early stopping (paramètre patience=1600), ce qui confirme que le modèle a atteint ses limites.\n",
    "\n",
    "Cette stabilisation du score indique clairement qu'un entraînement plus long n'apporterait pas d'amélioration significative, ce qui est le critère principal pour déterminer le nombre suffisant d'époques.\n",
    "\n",
    "De plus, si on observe la valeur \"LOSS TAGGER\" on peut apercevoir qu'au fil des epochs la valeur de l'erreur diminue sans pour autant que la valeur de la précision du Tagging (TAG_ACC) n'augmente significativement. Cela confirme que le modèle a atteint ses limites. En somme, nous pensons que 3-4 epochs sont largement suffisant. Nous avons le sentiment que si nous effectuons beaucoups plus d'epochs, nous allons favoriser le surapprentissage et donc une perte de généralisation.\n",
    "\n",
    "Le meilleur modèle se trouve dans le dossier ./myPOStagger1, où spaCy sauvegarde automatiquement la version qui a obtenu le meilleur score sur l'ensemble de validation pendant l'entraînement.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**4b.**  Veuillez charger le meilleur modèle (pipeline) dans la variable `nlp2` et afficher la *POS tagging accuracy* sur le corpus de test.  Le composant de la pipeline étant un *POS tagger*, vous devrez évaluer la propriété *tag_acc*. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-13T13:38:57.314235Z",
     "start_time": "2025-03-13T13:38:56.095190Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "POS tagging accuracy sur le corpus de test: 0.9323\n"
     ]
    }
   ],
   "source": [
    "# on charge le meilleur modèle depuis le dossier myPOStagger1\n",
    "nlp2 = spacy.load(\"./myPOStagger1/model-best\")\n",
    "\n",
    "# on chargee les données de test\n",
    "test_data = list(test_data.get_docs(nlp2.vocab))\n",
    "\n",
    "# Initialisation du scorer\n",
    "scorer = Scorer()\n",
    "\n",
    "# exemples pour l'évaluation\n",
    "examples = []\n",
    "for doc_correct in test_data:\n",
    "    doc_pred = nlp2(doc_correct.text)\n",
    "\n",
    "    from spacy.training import Example\n",
    "    example = Example(doc_pred, doc_correct)\n",
    "    examples.append(example)\n",
    "\n",
    "# Calcule du scores\n",
    "scores = scorer.score(examples)\n",
    "\n",
    "# Affichage de la POS tagging accuracy (tag_acc)\n",
    "print(f\"POS tagging accuracy sur le corpus de test: {scores['tag_acc']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Entraîner puis évaluer un POS tagger pour le français dans NLTK\n",
    "\n",
    "Le but de cette partie est d'utiliser le POS tagger appelé *Averaged Perceptron* fourni par NLTK, en l'entraînant pour le français sur les mêmes données que ci-dessus, importées cette fois-ci avec NLTK.  Pour une introduction au POS tagging avec NLTK, voir le [Chapitre 5.1 du livre NLTK](http://www.nltk.org/book/ch05.html).\n",
    "\n",
    "Remarques :\n",
    "* pour l'anglais, des taggers pré-entraînés sont disponibles dans NLTK ;\n",
    "* pour appliquer un tagger existant, on écrit `nltk.pos_tag(sentence)` où `sentence` est une liste de tokens et on obtient des paires (token, TAG) ;\n",
    "* l'implémentation de *Averaged Perceptron* a été faite par [Mathew Honnibal de Explosion.AI](https://explosion.ai/blog/part-of-speech-pos-tagger-in-python), la société qui a créé spaCy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**5a.** Veuillez charger les données d'entraînement et celles de test grâce à la classe `ConllCorpusReader` de NLTK.  [La documentation de cette classe](https://www.nltk.org/api/nltk.corpus.reader.conll.html#nltk.corpus.reader.conll.ConllCorpusReader) vous montrera comment indiquer les colonnes qui contiennent les tokens ('words') et les tags corrects ('pos').  Une fois les données chargées dans une variable, vous pouvez accéder aux phrases et aux tags avec la méthode `.tagged_sents()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-11T07:48:17.098184Z",
     "start_time": "2025-03-11T07:48:17.093323Z"
    }
   },
   "outputs": [],
   "source": [
    "from nltk.corpus.reader.conll import ConllCorpusReader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-11T07:48:28.477736Z",
     "start_time": "2025-03-11T07:48:28.463733Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nombre de phrases d'entraînement: 14554\n",
      "Nombre de phrases de test: 416\n",
      "\n",
      "Exemple de phrase annotée:\n",
      "[('Les', 'DET'), ('commotions', 'NOUN'), ('cérébrales', 'ADJ'), ('sont', 'AUX'), ('devenu', 'VERB'), ('si', 'ADV'), ('courantes', 'ADJ'), ('dans', 'ADP'), ('ce', 'DET'), ('sport', 'NOUN'), (\"qu'\", 'SCONJ'), ('on', 'PRON'), ('les', 'PRON'), ('considére', 'VERB'), ('presque', 'ADV'), ('comme', 'ADP'), ('la', 'DET'), ('routine', 'NOUN'), ('.', 'PUNCT')]\n"
     ]
    }
   ],
   "source": [
    "# Charger les données d'entraînement et de test\n",
    "train_sents = ConllCorpusReader(\"./input_data\", \"fr-ud-train.conllu\", columntypes=('ignore', 'words', 'ignore', 'pos'), separator=\"\\t\").tagged_sents()\n",
    "test_sents = ConllCorpusReader(\"./input_data\", \"fr-ud-test.conllu\", columntypes=('ignore', 'words', 'ignore', 'pos'), separator=\"\\t\").tagged_sents()\n",
    "\n",
    "# Afficher quelques informations sur les données chargées\n",
    "print(f\"Nombre de phrases d'entraînement: {len(train_sents)}\")\n",
    "print(f\"Nombre de phrases de test: {len(test_sents)}\")\n",
    "\n",
    "# Afficher un exemple de phrase annotée\n",
    "print(\"\\nExemple de phrase annotée:\")\n",
    "print(train_sents[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**5b.** Pour entraîner un POS tagger du type Averaged Perceptron, vous utiliserez le sous-module `nltk.tag.perceptron` du [module NLTK contenant les taggers](http://www.nltk.org/api/nltk.tag.html).  Les fonctions d'entraînement et de test sont documentées dans ce module.  Après l'entraînement, le réseau de neurones est enregistré dans un fichier `.pickle`, qui est écrasé à chaque entraînement si vous n'en faites pas une copie.  On peut également lire un fichier `.pickle` dans un tagger.\n",
    "\n",
    "Veuillez écrire le code pour entraîner le POS tagger sur les données d'entraînement.  Comme au (4), pensez augmenter graduellement le nombre d'époques (appelées 'itérations' dans NLTK).\n",
    "\n",
    "Combien de temps prend l'entraînement ?  Quelle est la taille du fichier enregistré ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-11T07:46:16.281894Z",
     "start_time": "2025-03-11T07:46:14.862607Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\jonas\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import nltk\n",
    "nltk.download('averaged_perceptron_tagger') # à exécuter la première fois\n",
    "from perceptron_patched import PerceptronTagger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-11T07:46:21.756474Z",
     "start_time": "2025-03-11T07:46:21.750283Z"
    }
   },
   "outputs": [],
   "source": [
    "ptagger = PerceptronTagger(load=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-11T07:48:07.873549Z",
     "start_time": "2025-03-11T07:48:07.177272Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Début de l'entraînement du PerceptronTagger avec 1 itération(s)...\n",
      "Entraînement terminé en 12.84 secondes\n",
      "Taille du fichier enregistré : 3,727,068 bytes (3.55 MB)\n",
      "----------------------------------------\n",
      "Début de l'entraînement du PerceptronTagger avec 2 itération(s)...\n",
      "Entraînement terminé en 21.82 secondes\n",
      "Taille du fichier enregistré : 4,653,899 bytes (4.44 MB)\n",
      "----------------------------------------\n",
      "Début de l'entraînement du PerceptronTagger avec 3 itération(s)...\n",
      "Entraînement terminé en 31.00 secondes\n",
      "Taille du fichier enregistré : 5,164,614 bytes (4.93 MB)\n",
      "----------------------------------------\n",
      "Début de l'entraînement du PerceptronTagger avec 5 itération(s)...\n",
      "Entraînement terminé en 50.82 secondes\n",
      "Taille du fichier enregistré : 5,708,395 bytes (5.44 MB)\n",
      "----------------------------------------\n",
      "Début de l'entraînement du PerceptronTagger avec 10 itération(s)...\n",
      "Entraînement terminé en 100.12 secondes\n",
      "Taille du fichier enregistré : 6,223,411 bytes (5.94 MB)\n",
      "----------------------------------------\n",
      "Début de l'entraînement du PerceptronTagger avec 20 itération(s)...\n",
      "Entraînement terminé en 179.01 secondes\n",
      "Taille du fichier enregistré : 6,595,177 bytes (6.29 MB)\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "BASE_PATH = \"./my_perceptron_tagger/\"\n",
    "ITERATIONS = [1, 2, 3, 5, 10, 20]\n",
    "\n",
    "# Start timing\n",
    "for iters in ITERATIONS:\n",
    "    ptagger = PerceptronTagger(load=False)\n",
    "    start_time = time.time()\n",
    "    print(f\"Début de l'entraînement du PerceptronTagger avec {iters} itération(s)...\")\n",
    "    \n",
    "    full_path = BASE_PATH + f\"{iters}_iterations\"\n",
    "\n",
    "    # Entraînement avec iters itérations\n",
    "    ptagger.train(train_sents, save_loc=full_path, nr_iter=iters)\n",
    "    \n",
    "    # Calculer le temps d'entraînement\n",
    "    training_time = time.time() - start_time\n",
    "    print(f\"Entraînement terminé en {training_time:.2f} secondes\")\n",
    "    \n",
    "    # Taille du fichier enregistré\n",
    "    file_size = os.path.getsize(full_path + \"_averaged_perceptron_tagger.xxx.weights.json\") \n",
    "    print(f\"Taille du fichier enregistré : {file_size:,} bytes ({file_size/1024/1024:.2f} MB)\")\n",
    "    print(\"-\" * 40)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-10T14:55:52.152751Z",
     "start_time": "2025-03-10T14:55:52.147747Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nDébut de l'entraînement du PerceptronTagger avec 1 itération(s)...\\nEntraînement terminé en 11.18 secondes\\nTaille du fichier enregistré : 3,727,068 bytes (3.55 MB)\\n----------------------------------------\\nDébut de l'entraînement du PerceptronTagger avec 2 itération(s)...\\nEntraînement terminé en 20.24 secondes\\nTaille du fichier enregistré : 4,667,656 bytes (4.45 MB)\\n----------------------------------------\\nDébut de l'entraînement du PerceptronTagger avec 3 itération(s)...\\nEntraînement terminé en 28.75 secondes\\nTaille du fichier enregistré : 5,180,443 bytes (4.94 MB)\\n----------------------------------------\\nDébut de l'entraînement du PerceptronTagger avec 5 itération(s)...\\nEntraînement terminé en 46.32 secondes\\nTaille du fichier enregistré : 5,713,140 bytes (5.45 MB)\\n----------------------------------------\\nDébut de l'entraînement du PerceptronTagger avec 10 itération(s)...\\nEntraînement terminé en 93.20 secondes\\nTaille du fichier enregistré : 6,242,812 bytes (5.95 MB)\\n----------------------------------------\\nDébut de l'entraînement du PerceptronTagger avec 20 itération(s)...\\nEntraînement terminé en 180.89 secondes\\nTaille du fichier enregistré : 6,580,806 bytes (6.28 MB)\\n----------------------------------------\\n\\n\\nNous observons que le temps d'entraînement et la taille su model augmente de façon linéaire avec le nombre d'itérations.\\n\\n\""
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Veuillez écrire ici vos réponses aux questions (temps d'entraînement et taille du modèle).\n",
    "\n",
    "\"\"\"\n",
    "Début de l'entraînement du PerceptronTagger avec 1 itération(s)...\n",
    "Entraînement terminé en 11.18 secondes\n",
    "Taille du fichier enregistré : 3,727,068 bytes (3.55 MB)\n",
    "----------------------------------------\n",
    "Début de l'entraînement du PerceptronTagger avec 2 itération(s)...\n",
    "Entraînement terminé en 20.24 secondes\n",
    "Taille du fichier enregistré : 4,667,656 bytes (4.45 MB)\n",
    "----------------------------------------\n",
    "Début de l'entraînement du PerceptronTagger avec 3 itération(s)...\n",
    "Entraînement terminé en 28.75 secondes\n",
    "Taille du fichier enregistré : 5,180,443 bytes (4.94 MB)\n",
    "----------------------------------------\n",
    "Début de l'entraînement du PerceptronTagger avec 5 itération(s)...\n",
    "Entraînement terminé en 46.32 secondes\n",
    "Taille du fichier enregistré : 5,713,140 bytes (5.45 MB)\n",
    "----------------------------------------\n",
    "Début de l'entraînement du PerceptronTagger avec 10 itération(s)...\n",
    "Entraînement terminé en 93.20 secondes\n",
    "Taille du fichier enregistré : 6,242,812 bytes (5.95 MB)\n",
    "----------------------------------------\n",
    "Début de l'entraînement du PerceptronTagger avec 20 itération(s)...\n",
    "Entraînement terminé en 180.89 secondes\n",
    "Taille du fichier enregistré : 6,580,806 bytes (6.28 MB)\n",
    "----------------------------------------\n",
    "\n",
    "\n",
    "Nous observons que le temps d'entraînement et la taille du model augmentent linéairement avec le nombre d'itérations.\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**5c.** Veuillez évaluer le tagger sur les données de test et afficher le taux de correction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-10T14:55:52.193650Z",
     "start_time": "2025-03-10T14:55:52.189354Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modèle entraîné avec 1 itérations :\n",
      "Précision du tagger sur les données de test: 0.9522\n",
      "\n",
      "Modèle entraîné avec 2 itérations :\n",
      "Précision du tagger sur les données de test: 0.9565\n",
      "\n",
      "Modèle entraîné avec 3 itérations :\n",
      "Précision du tagger sur les données de test: 0.9571\n",
      "\n",
      "Modèle entraîné avec 5 itérations :\n",
      "Précision du tagger sur les données de test: 0.9600\n",
      "\n",
      "Modèle entraîné avec 10 itérations :\n",
      "Précision du tagger sur les données de test: 0.9604\n",
      "\n",
      "Modèle entraîné avec 20 itérations :\n",
      "Précision du tagger sur les données de test: 0.9602\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for iterations in ITERATIONS:\n",
    "    print(f\"Modèle entraîné avec {iterations} itérations :\")\n",
    "    # Charger le modèle et vérifier les classes\n",
    "    full_path = BASE_PATH + f\"/{iterations}_iterations\"\n",
    "    ptagger.load_from_json(full_path)\n",
    "    \n",
    "    accuracy = ptagger.accuracy(test_sents)\n",
    "    print(f\"Précision du tagger sur les données de test: {accuracy:.4f}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Conclusion\n",
    "\n",
    "Veuillez remplir le tableau suivant avec les scores obtenus et discuter brièvement comment se comparent les trois POS taggers sur ces données de test.\n",
    "\n",
    "| spaCy (partie 3) | spaCy (partie 4) | NLTK (partie 5) | \n",
    "|------------------|------------------|-----------------|\n",
    "| tagger fourni    | tagger entraîné  | tagger entraîné |\n",
    "| 0.9177           | 0.9340           | 0.9602          |\n",
    "\n",
    "Les résultats montrent que les trois approches donnent de bonnes performances sur ces données de test. Le tagger pré-entraîné de spaCy atteint environ 91,77 %, tandis que le tagger spaCy entraîné obtient une précision de 93,40 %. Le tagger entraîné via NLTK affiche la meilleure performance avec 96,02 %. \n",
    "\n",
    "Ces différences indiquent qu'un entraînement personnalisé, que ce soit avec spaCy ou NLTK, permet d'améliorer la précision par rapport au modèle pré-entraîné. Toutefois, le choix entre les deux approches entraînées pourra dépendre d'autres facteurs tels que la facilité d'intégration, la vitesse d'exécution ou encore la gestion des erreurs spécifiques aux données utilisées.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Fin du Labo.** Veuillez nettoyer ce notebook en gardant seulement les résultats désirés, l'enregistrer, et le soumettre comme devoir sur Cyberlearn."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
